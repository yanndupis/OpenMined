{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-30 08:38:36,316] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from syft.controller import tensors, models\n",
    "\n",
    "\n",
    "import syft\n",
    "import syft.interfaces.torch.actual_torch as actual_torch\n",
    "import syft.interfaces.torch as torch\n",
    "import syft.interfaces.torch.nn as nn\n",
    "import syft.interfaces.torch.nn.functional as F\n",
    "import syft.interfaces.torch.optim as optim\n",
    "from syft.interfaces.torch.autograd import Variable\n",
    "from syft.interfaces.torch.distributions import Categorical\n",
    "\n",
    "# import torch as torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.autograd import Variable\n",
    "# from torch.distributions import Categorical\n",
    "\n",
    "gamma = 0.9\n",
    "seed = 543\n",
    "render = False\n",
    "log_interval = 100\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine = nn.Linear(4, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        action_scores = self.affine(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "    \n",
    "policy = Policy()\n",
    "\n",
    "cached_actions = [1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0]\n",
    "actions = list()\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(Variable(state))\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    \n",
    "    # testing purposes only - comment out when using actual pytorch\n",
    "    action = Variable(torch.IntTensor([cached_actions[len(actions)]]))\n",
    "    \n",
    "    actions.append(action)\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.data[0]\n",
    "\n",
    "optimizer = optim.SGD(policy.parameters(), lr=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_reward = 10\n",
    "\n",
    "state = env.reset()\n",
    "for t in range(10000):  # Don't infinite loop while learning\n",
    "    action = select_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if render:\n",
    "        env.render()\n",
    "    policy.rewards.append(reward)\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "running_reward = running_reward * 0.99 + t * 0.01\n",
    "\n",
    "R = 0\n",
    "policy_loss = []\n",
    "rewards = []\n",
    "for r in policy.rewards[::-1]:\n",
    "    R = r + gamma * R\n",
    "    rewards.insert(0, R)\n",
    "    \n",
    "rewards = torch.Tensor(rewards)\n",
    "rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "for log_prob, reward in zip(policy.saved_log_probs, rewards):\n",
    "    policy_loss.append(-log_prob * reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[ 1.085435]]\n",
       " [syft.FloatTensor:1081 grad:None size:1x1 c:[] p:[1080, 1082] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1080 grad:None size:1x1 c:[1081] p:[919] init:neg]\n",
       " \t[syft.FloatTensor:1082 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[ 0.6652827]]\n",
       " [syft.FloatTensor:1084 grad:None size:1x1 c:[] p:[1083, 1085] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1083 grad:None size:1x1 c:[1084] p:[928] init:neg]\n",
       " \t[syft.FloatTensor:1085 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[ 0.930799]]\n",
       " [syft.FloatTensor:1087 grad:None size:1x1 c:[] p:[1086, 1088] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1086 grad:None size:1x1 c:[1087] p:[937] init:neg]\n",
       " \t[syft.FloatTensor:1088 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[ 0.5522543]]\n",
       " [syft.FloatTensor:1090 grad:None size:1x1 c:[] p:[1089, 1091] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1089 grad:None size:1x1 c:[1090] p:[946] init:neg]\n",
       " \t[syft.FloatTensor:1091 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[ 0.4240058]]\n",
       " [syft.FloatTensor:1093 grad:None size:1x1 c:[] p:[1092, 1094] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1092 grad:None size:1x1 c:[1093] p:[955] init:neg]\n",
       " \t[syft.FloatTensor:1094 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[ 0.3128518]]\n",
       " [syft.FloatTensor:1096 grad:None size:1x1 c:[] p:[1095, 1097] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1095 grad:None size:1x1 c:[1096] p:[964] init:neg]\n",
       " \t[syft.FloatTensor:1097 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[ 0.2174359]]\n",
       " [syft.FloatTensor:1099 grad:None size:1x1 c:[] p:[1098, 1100] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1098 grad:None size:1x1 c:[1099] p:[973] init:neg]\n",
       " \t[syft.FloatTensor:1100 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[ 0.1363575]]\n",
       " [syft.FloatTensor:1102 grad:None size:1x1 c:[] p:[1101, 1103] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1101 grad:None size:1x1 c:[1102] p:[982] init:neg]\n",
       " \t[syft.FloatTensor:1103 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[ 0.3236172]]\n",
       " [syft.FloatTensor:1105 grad:None size:1x1 c:[] p:[1104, 1106] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1104 grad:None size:1x1 c:[1105] p:[991] init:neg]\n",
       " \t[syft.FloatTensor:1106 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[ 0.05800766]]\n",
       " [syft.FloatTensor:1108 grad:None size:1x1 c:[] p:[1107, 1109] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1107 grad:None size:1x1 c:[1108] p:[1000] init:neg]\n",
       " \t[syft.FloatTensor:1109 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[-0.0641876]]\n",
       " [syft.FloatTensor:1111 grad:None size:1x1 c:[] p:[1110, 1112] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1110 grad:None size:1x1 c:[1111] p:[1009] init:neg]\n",
       " \t[syft.FloatTensor:1112 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[-0.1344222]]\n",
       " [syft.FloatTensor:1114 grad:None size:1x1 c:[] p:[1113, 1115] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1113 grad:None size:1x1 c:[1114] p:[1018] init:neg]\n",
       " \t[syft.FloatTensor:1115 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[-0.191061]]\n",
       " [syft.FloatTensor:1117 grad:None size:1x1 c:[] p:[1116, 1118] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1116 grad:None size:1x1 c:[1117] p:[1027] init:neg]\n",
       " \t[syft.FloatTensor:1118 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[-0.2356962]]\n",
       " [syft.FloatTensor:1120 grad:None size:1x1 c:[] p:[1119, 1121] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1119 grad:None size:1x1 c:[1120] p:[1036] init:neg]\n",
       " \t[syft.FloatTensor:1121 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[-2.071509]]\n",
       " [syft.FloatTensor:1123 grad:None size:1x1 c:[] p:[1122, 1124] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1122 grad:None size:1x1 c:[1123] p:[1045] init:neg]\n",
       " \t[syft.FloatTensor:1124 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[-0.4059561]]\n",
       " [syft.FloatTensor:1126 grad:None size:1x1 c:[] p:[1125, 1127] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1125 grad:None size:1x1 c:[1126] p:[1054] init:neg]\n",
       " \t[syft.FloatTensor:1127 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " , [[-0.4310972]]\n",
       " [syft.FloatTensor:1129 grad:None size:1x1 c:[] p:[1128, 1130] init:mul_scalar]\n",
       " \n",
       " \t-----------creators-----------\n",
       " \t[syft.FloatTensor:1128 grad:None size:1x1 c:[1129] p:[1063] init:neg]\n",
       " \t[syft.FloatTensor:1130 grad:None size:1 c:[] p:[] init:mul_scalar]\n",
       " \t------------------------------\n",
       " \n",
       " ]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-25 14:32:57,551] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tLast length:    98\tAverage length: 66.12\n",
      "Episode 200\tLast length:    17\tAverage length: 85.61\n",
      "Episode 300\tLast length:    49\tAverage length: 80.06\n",
      "Episode 400\tLast length:   169\tAverage length: 138.29\n",
      "Episode 500\tLast length:    41\tAverage length: 135.51\n",
      "Episode 600\tLast length:    88\tAverage length: 126.97\n",
      "Episode 700\tLast length:   177\tAverage length: 163.88\n",
      "Episode 800\tLast length:   126\tAverage length: 179.32\n",
      "Episode 900\tLast length:    63\tAverage length: 138.71\n",
      "Episode 1000\tLast length:   199\tAverage length: 139.44\n",
      "Episode 1100\tLast length:   199\tAverage length: 176.30\n",
      "Episode 1200\tLast length:   199\tAverage length: 189.80\n",
      "Solved! Running reward is now 195.00467036348365 and the last episode runs to 199 time steps!\n"
     ]
    }
   ],
   "source": [
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "\n",
    "running_reward = 10\n",
    "for i_episode in count(1):\n",
    "    state = env.reset()\n",
    "    for t in range(10000):  # Don't infinite loop while learning\n",
    "        action = select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if render:\n",
    "            env.render()\n",
    "        policy.rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    running_reward = running_reward * 0.99 + t * 0.01\n",
    "    finish_episode()\n",
    "    if i_episode % log_interval == 0:\n",
    "        print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "            i_episode, t, running_reward))\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
