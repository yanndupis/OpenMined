{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-25 14:32:57,551] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tLast length:    98\tAverage length: 66.12\n",
      "Episode 200\tLast length:    17\tAverage length: 85.61\n",
      "Episode 300\tLast length:    49\tAverage length: 80.06\n",
      "Episode 400\tLast length:   169\tAverage length: 138.29\n",
      "Episode 500\tLast length:    41\tAverage length: 135.51\n",
      "Episode 600\tLast length:    88\tAverage length: 126.97\n",
      "Episode 700\tLast length:   177\tAverage length: 163.88\n",
      "Episode 800\tLast length:   126\tAverage length: 179.32\n",
      "Episode 900\tLast length:    63\tAverage length: 138.71\n",
      "Episode 1000\tLast length:   199\tAverage length: 139.44\n",
      "Episode 1100\tLast length:   199\tAverage length: 176.30\n",
      "Episode 1200\tLast length:   199\tAverage length: 189.80\n",
      "Solved! Running reward is now 195.00467036348365 and the last episode runs to 199 time steps!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "gamma = 0.9\n",
    "seed = 543\n",
    "render = False\n",
    "log_interval = 100\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine2 = nn.Linear(4, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.SGD(policy.parameters(), lr=0.15)\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(Variable(state))\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.data[0]\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "\n",
    "\n",
    "running_reward = 10\n",
    "for i_episode in count(1):\n",
    "    state = env.reset()\n",
    "    for t in range(10000):  # Don't infinite loop while learning\n",
    "        action = select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if render:\n",
    "            env.render()\n",
    "        policy.rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    running_reward = running_reward * 0.99 + t * 0.01\n",
    "    finish_episode()\n",
    "    if i_episode % log_interval == 0:\n",
    "        print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "            i_episode, t, running_reward))\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
