using System;
using System.Collections.Generic;

namespace OpenMined.Syft.Tensor
{
    public partial class FloatTensor
    {
        private bool autograd;
        public FloatTensor Grad { get; private set; }

        private bool keepgrads;

        private List<FloatTensor> creators;
        private string creation_op;
        private Dictionary<int, int> children;

        public void InitAutograd()
        {
//			if(!autograd) {
            autograd = true;
            creators = new List<FloatTensor>();
            children = new Dictionary<int, int>();
//			}
        }

        public bool AllChildrenGradsAccountedFor()
        {
            foreach (var item in children)
            {
                if (item.Value == 0)
                {
                    return false;
                }
            }
            return true;
        }


        // hook autograd two parents - one scalar
        public void HookAutograd(ref FloatTensor result, float x, string creation_op)
        {
            if (autograd)
            {
                FloatTensor new_child =
                    new FloatTensor(_controller: controller, _shape: new int[] {1}, _data: new float[] {x});

                result.InitAutograd();
                result.creators.Add(this);
                result.creators.Add(new_child);
                result.creation_op = creation_op;

                children.Add(result.Id, 0);
//				new_child.children.Add (result.Id, 0);
			}

		}

		// hook autograd two parents
		public void HookAutograd(ref FloatTensor result, ref FloatTensor x, string creation_op) {

			if (autograd) {

				result.InitAutograd ();
				result.creators.Add (this);
				result.creators.Add (x);
				result.creation_op = creation_op;

				children.Add (result.Id, 0);
				x.children.Add (result.Id, 0);

			}

		}

		// hook autograd single parent
		public void HookAutograd(ref FloatTensor result, string creation_op) {

			if (autograd) {

				result.InitAutograd ();
				result.creators.Add (this);
				result.creation_op = creation_op;

				children.Add (result.Id, 0);
			}
		}

	    public void Backward(FloatTensor grad = null, FloatTensor grad_origin = null)
	    {

		    if (autograd)
		    {
			    if (grad == null)
			    {
				    grad = this.controller.createOnesTensorLike(this);
				    grad.Autograd = false;
			    }

			    if (grad_origin != null)
			    {
				    if (children[grad_origin.Id] > 0)
				    {
					    throw new InvalidOperationException("Can't backprop more than once.");
				    }
				    else
				    {
					    children[grad_origin.Id] += 1;
				    }
			    }

			    if (this.Grad == null)
			    {
				    this.Grad = grad;
			    }
			    else
			    {
				    this.Grad.Add(grad, true);
			    }

			    // grads must not have grads of their own
			    if (this.Grad.autograd == true)
			    {
				    throw new InvalidOperationException("Sorry, grads cannot have grads");
			    }

			    // only continue backpropping if there's something to backprop into
			    // only continue backpropping if all gradients (from children) are accounted for
			    // override waiting for children if "backprop" was called on this variable directly
			    if (this.creators != null && this.creators.Count > 0 && (grad_origin == null || AllChildrenGradsAccountedFor()))
			    {
				    if (creation_op == "add_elem")
				    {

					    creators[0].Backward(grad.Copy(), this);
					    creators[1].Backward(grad.Copy(), this);

				    }
				    else if (creation_op == "mul_elem")
				    {

					    creators[0].Backward(grad.Mul(creators[1]), this);
					    creators[1].Backward(grad.Mul(creators[0]), this);

				    }
				    else if (creation_op == "div_elem")
				    {

					    creators[0].Backward(grad.Div(creators[1]), this);
					    creators[1].Backward(grad.Div(creators[0]), this);

				    }
				    else if (creation_op == "sub_elem")
				    {

					    creators[0].Backward(grad.Copy(), this);
					    creators[1].Backward(grad.Neg(), this);

				    }
				    else if (creation_op == "mm")
				    {

                        creators[0].Backward(grad.MM(creators[1].Transpose()), this);
                        creators[1].Backward(creators[0].Transpose().MM(grad), this);

				    }
				    else if (creation_op == "sigmoid")
				    {

					    FloatTensor c = this.Copy();
					    c.autograd = false;
					    creators[0].Backward(c.Neg().Add((float) 1).Mul(this).Mul(grad), this);

				    }
				    else if (creation_op == "pow_scalar")
				    {

					    FloatTensor self_nograd = creators[0].Copy();
					    self_nograd.autograd = false;
					    creators[0].Backward(self_nograd.Mul(grad).Mul(creators[1].Data[0]), this);

				    }

					/*if (!keepgrads) {
						controller.RemoveTensor (grad.id);
					}*/

			    }
		    }
	    }
    }
}